{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# 3 split the data into training (70%) and validation (30%)\n","**OBJECTIVE:** randomly split the annotated data into\n","- *training* (70% of the tiles): set of data used for learning (by the model), that is, to fit the parameters to the machine learning model.\n","- *validation* set (30%): Set of data used to provide an unbiased evaluation of a model fitted on the training dataset while tuning model hyperparameters.\n","Also play a role in other forms of model preparation, such as feature selection, threshold cut-off selection.\n","\n","Normally, one would create a third test dataset for a fully independent evaluation of model's performance on unseen data. In this course the test data are already taken out of the data and will be provided later in the course.\n","\n","\n","\n","**INPUT:**\n","- `path_to_tiles`=\"/content/drive/MyDrive/NOVA_course_deep_learning/data/tiles/10m_krakstad_202304_sun\"\n","-`split_train`= 0.7\n","\n","**OUTPUT:**\n","- train and validation data organized in the following folders:\n","\n","```\n","â”œâ”€â”€ train\n","â”‚   â”œâ”€â”€ images\n","â”‚   â””â”€â”€ labels\n","â”œâ”€â”€ val\n","â”‚   â”œâ”€â”€ images\n","â”‚   â””â”€â”€ labels\n","```\n"],"metadata":{"id":"vcYhX5JPvnUQ"}},{"cell_type":"code","source":["annotator_ID=17 # change this to your folder ID\n","\n","path_to_tiles=\"/content/drive/MyDrive/NOVA_course_deep_learning/data/annotated_data/train/\"+str(annotator_ID)\n","\n","# define split for training and validation\n","split_train= 0.7 #\n","split_val=1-split_train"],"metadata":{"id":"ESxfoDbRtkVj","executionInfo":{"status":"ok","timestamp":1687937358550,"user_tz":-120,"elapsed":3,"user":{"displayName":"Rorai Pereira Martins Neto","userId":"05628001015953149988"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### 3.1 Load libraries"],"metadata":{"id":"IOOTsKg9tf-D"}},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","# mount google drive\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KQ9Ofi8kt8AI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687937378432,"user_tz":-120,"elapsed":16832,"user":{"displayName":"Rorai Pereira Martins Neto","userId":"05628001015953149988"}},"outputId":"4538ea94-9708-4296-a9c8-f39994ccb2e4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### 3.2 Create train and validation directories and subdivide each into \"images\" and \"labels\" sub-directories"],"metadata":{"id":"6hLdXNOguHeA"}},{"cell_type":"code","source":["train_dir = os.path.join(path_to_tiles, \"train\")\n","os.makedirs(train_dir, exist_ok=True) # creates new directory for training data\n","val_dir = os.path.join(path_to_tiles, \"val\")\n","os.makedirs(val_dir, exist_ok=True) # creates new directory for validation data\n","val_img_dir = os.path.join(path_to_tiles, \"val\",\"images\")\n","os.makedirs(val_img_dir, exist_ok=True) # creates new directory for training data\n","train_img_dir = os.path.join(path_to_tiles, \"train\",\"images\")\n","os.makedirs(train_img_dir, exist_ok=True) # creates new directory for training data\n","val_ann_dir = os.path.join(path_to_tiles, \"val\",\"labels\")\n","os.makedirs(val_ann_dir, exist_ok=True) # creates new directory for training data\n","train_ann_dir = os.path.join(path_to_tiles, \"train\",\"labels\")\n","os.makedirs(train_ann_dir, exist_ok=True) # creates new directory for training data\n"],"metadata":{"id":"EO4_BRQauMrb","executionInfo":{"status":"ok","timestamp":1687937387130,"user_tz":-120,"elapsed":2098,"user":{"displayName":"Rorai Pereira Martins Neto","userId":"05628001015953149988"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Randomly sample tiles"],"metadata":{"id":"RpooLhCauWx_"}},{"cell_type":"code","source":["# Get a list of all the .txt files in the data directory\n","txt_files = [f for f in os.listdir(path_to_tiles) if f.endswith(\".txt\")]\n","img_files = [f for f in os.listdir(path_to_tiles) if f.endswith(\".tif\")]"],"metadata":{"id":"sAH2eQwTF-fQ","executionInfo":{"status":"ok","timestamp":1687937393037,"user_tz":-120,"elapsed":211,"user":{"displayName":"Rorai Pereira Martins Neto","userId":"05628001015953149988"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# remove .txt files that have no image (not sure why ?)\n","txt_files_with_tif = []\n","for txt_file in txt_files:\n","    # get the base name of the text file\n","    txt_base_name = os.path.basename(txt_file)\n","    # replace the file extension with .tif to get the corresponding tif file name\n","    img_file = os.path.join(os.path.dirname(txt_file), os.path.splitext(txt_base_name)[0] + '.tif')\n","    img_file=path_to_tiles+\"/\"+img_file\n","    #print(\"txt: \"+txt_file)\n","    #print(\"tif: \"+img_file)\n","    # check if the tif file exists\n","    if os.path.exists(img_file):\n","      #print(\"path to image \" + img_file + \" does not exist!\")\n","      txt_files_with_tif.append(txt_file)\n","\n"],"metadata":{"id":"l6_I5DQEDd0e","executionInfo":{"status":"ok","timestamp":1687937396852,"user_tz":-120,"elapsed":209,"user":{"displayName":"Rorai Pereira Martins Neto","userId":"05628001015953149988"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["txt_files=txt_files_with_tif\n","\n","# Shuffle the list of text files\n","random.shuffle(txt_files)\n","#train=random.sample(txt_files, )\n","\n","# Calculate the number of files for the train and validation sets\n","train_size = int(0.7 * len(txt_files))\n","val_size = len(txt_files) - train_size"],"metadata":{"id":"2Olsju2esPOl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Move the text annotation files and respective images to the train and validation directories"],"metadata":{"id":"DwIl8JDcyBey"}},{"cell_type":"code","source":["# iterate through each annotated .txt file\n","for i, txt_file in enumerate(txt_files):\n","    if i < train_size:\n","        dest_dir = train_dir\n","    else:\n","        dest_dir = val_dir\n","    #print(\"path to \"+path_to_tiles+\"/\"+txt_file+\" exists: \"+ str(os.path.exists(txt_file)))\n","    if os.path.exists(path_to_tiles+\"/\"+txt_file):\n","      src_file = os.path.join(path_to_tiles, txt_file)\n","      src_img = os.path.join(path_to_tiles, os.path.splitext(txt_file)[0]+\".tif\")\n","      if os.path.exists(src_img):\n","        dest_file = os.path.join(dest_dir,\"labels\", txt_file)\n","        dest_img = os.path.join(dest_dir,\"images\", os.path.splitext(txt_file)[0]+\".tif\")\n","        #print(\"copying files\")\n","        shutil.move(src_file, dest_file)\n","        shutil.move(src_img, dest_img)"],"metadata":{"id":"6sGgm68syA51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It is often also good practice to add approximately 10% of background images, i.e. that do not contain any bounding box. This will help the model to avoid to produce odd detections in areas otherwise unseen to the model.\n","\n","A simple (and efficient) way to do so it to scroll throught the image tiles (in google drive), select them manually and copying 70% of them in the training and 30% in the validation folders."],"metadata":{"id":"Cs1UJMrhyZA6"}},{"cell_type":"markdown","source":["# The end. And now let's get to the fun part ðŸ¥³ to the [model training](https://colab.research.google.com/drive/1dZ4uJHNhjbMCdk0pSyhskkA7It1jKlnF)\n"],"metadata":{"id":"l3ZaRHMmx3o7"}},{"cell_type":"code","source":[],"metadata":{"id":"bqAsjmSZQfyH"},"execution_count":null,"outputs":[]}]}